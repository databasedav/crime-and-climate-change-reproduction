{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Creation\n",
    "\n",
    "Requires local Nominatim server covering the continental United States at the minimum. Because tens of thousands of reverse geolocation queries must be made and then organized, the code below does many operations asynchronously. See https://ipython-books.github.io/59-distributing-python-code-across-multiple-cores-with-ipython/ and https://ipython-books.github.io/510-interacting-with-asynchronous-parallel-tasks-in-ipython/ for setup and more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import _pickle as pickle  # using cPickle\n",
    "import urllib\n",
    "import datetime\n",
    "import ipyparallel\n",
    "import ipywidgets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from geopy import Point, distance\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderServiceError, GeocoderTimedOut\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate to Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_to_zip(coord):\n",
    "    \"\"\"\n",
    "    Returns zip code from entered coordinate. Zip code\n",
    "    is easier to map to fips code because otherwise requires\n",
    "    both state and county and county names are often\n",
    "    ambiguous and require manually editting to allow mapping\n",
    "    to fips (e.g. St. to Saint or vice versa).\n",
    "    coord: tuple consisting of (latitude, longitude)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time.sleep(0)\n",
    "        raw = geolocator.reverse(coord, timeout=15).raw\n",
    "        # occurs when coord is outside available region\n",
    "        if 'error' in raw:\n",
    "            return (coord, None)\n",
    "        # occurs when coord is not in the United States\n",
    "        if raw['address']['country_code'] != 'us':\n",
    "            return (coord, None)\n",
    "        return (coord, raw['address']['postcode'])\n",
    "    # occurs when address, or postcode is not available\n",
    "    except KeyError as e:\n",
    "        return (coord, None)\n",
    "    # catch all for potential geopy exceptions but ideally should not occur\n",
    "    except Exception as e:\n",
    "        return (coord, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_generator(min_lat, max_lat, min_lon, max_lon, dist):\n",
    "    \"\"\"\n",
    "    Generates (latitude, longitude) tuples bounded by the given\n",
    "    minimum and maximum latitudes and longitudes.\n",
    "    \"\"\"\n",
    "    start = Point(max_lat, min_lon)\n",
    "    point = start\n",
    "    \n",
    "    def point_to_tuple(point):\n",
    "        return (point.latitude, point.longitude)\n",
    "    \n",
    "    coord = point_to_tuple(point)\n",
    "    ver_dist = distance.distance(miles=dist)\n",
    "    hor_dist = distance.distance(miles=dist)\n",
    "    coords = list() # list of coords forming the grid\n",
    "    \n",
    "    while True:\n",
    "        # eastmost bound reached\n",
    "        if coord[1] > max_lon:\n",
    "            # go back to starting point and move south\n",
    "            point = ver_dist.destination(point=start, bearing=180)\n",
    "            # increment vertical distance\n",
    "            ver_dist += distance.distance(miles=5)\n",
    "        # southmost bound reached\n",
    "        elif coord[0] < min_lat:\n",
    "            break\n",
    "        yield coord\n",
    "        # move east\n",
    "        point = hor_dist.destination(point=point, bearing=90)\n",
    "        coord = point_to_tuple(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(domain='localhost:8080/nominatim', scheme='http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator.reverse((37.873522, -122.257692), timeout=15).raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://en.wikipedia.org/wiki/List_of_extreme_points_of_the_United_States, for the 48 contiguous states:\n",
    "* northernmost latitude: 49°23′04.1″\n",
    "* southernmost latitude: 24°26.8′\n",
    "* westermost longitude: -124°47.1′\n",
    "* easternmost longitude: -66°56′49.3″"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_lat = 24\n",
    "max_lat= 50\n",
    "min_lon = -125\n",
    "max_lon = -66\n",
    "dist = 5\n",
    "coords = list(coordinate_generator(min_lat, max_lat, min_lon, max_lon, dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "rc = ipyparallel.Client()\n",
    "view = rc.load_balanced_view()\n",
    "\n",
    "with rc[:].sync_imports():\n",
    "    import time\n",
    "\n",
    "rc[:].push(dict(\n",
    "    geolocator=geolocator\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = view.map_async(coord_to_zip, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coords = len(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctrl + enter on this cell to see progress without blocking kernel\n",
    "# takes around 15 minutes with 12 logical processors (i7 8700k) and 16gb ram\n",
    "print(f'Time Elapsed: {datetime.timedelta(seconds=math.ceil(ar.elapsed))}')\n",
    "print(f'Coordinates Completed: {ar.progress}/{num_coords}')\n",
    "print(f'Completed {round(ar.serial_time/ar.wall_time, 2)}x faster than serial computation' if ar.ready() else 'Still Running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will block the kernel; interrupt the kernel to unblock\n",
    "ar.wait_interactive()\n",
    "print(f'Completed {round(ar.serial_time/ar.wall_time, 2)}x faster than serial computation' if ar.ready() else 'Still Running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_zip_dict = dict()\n",
    "exception_coords = list()\n",
    "for i, (coord, maybe_zip) in enumerate(ar):\n",
    "    print(f'processing grid points: {i+1}/{num_coords}')\n",
    "    clear_output(wait=True)\n",
    "    if isinstance(maybe_zip, Exception):\n",
    "        exception_coords.append(coord)\n",
    "    elif maybe_zip is not None:\n",
    "        coord_zip_dict[coord] = maybe_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideally none of the coords should raise exceptions\n",
    "# in the case there are, try to manually rerun them\n",
    "exception_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/coord_zip_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(coord_zip_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create slightly different grids for testing pipeline robustness\n",
    "# min_lat = 24\n",
    "# max_lon = -66\n",
    "# max_lat_options = [50 + 0.1 * i for i in range(1, 11)]\n",
    "# min_lon_options = [-125 - 0.1 * i for i in range(1, 11)]\n",
    "# dist = 5\n",
    "# exception_coords = list()\n",
    "\n",
    "# for i, max_lat in enumerate(max_lat_options):\n",
    "#     for j, min_lon in enumerate(min_lon_options):\n",
    "#         print(f'generating grid: {i*10+j+1}/100')\n",
    "#         coords = list(coordinate_generator(min_lat, max_lat, min_lon, max_lon, dist))\n",
    "#         ar = view.map_async(coord_to_zip, coords)\n",
    "#         view.shutdown()\n",
    "#         break\n",
    "# #         progress_bar(ar)\n",
    "#         coord_zip_dict = dict()\n",
    "#         num_coords = len(coords)\n",
    "#         clear_output(wait=True)\n",
    "#         for k, (coord, maybe_zip) in enumerate(ar):\n",
    "#             print(f'grid: {i*10+j+1}/100\\nprocessing grid points: {k+1}/{num_coords}')\n",
    "#             clear_output(wait=True)\n",
    "#             if isinstance(maybe_zip, Exception):\n",
    "#                 exception_coords.append(coord)\n",
    "#             elif maybe_zip is not None:\n",
    "#                 coord_zip_dict[coord] = maybe_zip\n",
    "#         with open(f'pickles/coord_zip_dict{i}{j}.pkl', 'wb') as file:\n",
    "#             pickle.dump(coord_zip_dict, file)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exception_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip Coordinates to FIPS Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/coord_zip_dict.pkl', 'rb') as file:\n",
    "    coord_zip_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter bad zip codes and cut zip+4's\n",
    "coord_zip_dict = {coord: zip_code if len(zip_code) == 5 else zip_code[:5] for coord, zip_code in coord_zip_dict.items() if len(zip_code) >= 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop is fast and need not be pickled\n",
    "# dict from county name to list of coordinates within said county\n",
    "zip_coords_dict = dict()\n",
    "for coord, z in coord_zip_dict.items():\n",
    "    try:\n",
    "        zip_coords_dict[z].append(coord)\n",
    "    except KeyError:\n",
    "        zip_coords_dict[z] = [coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from https://www.kaggle.com/danofer/zipcodes-county-fips-crosswalk/version/1#\n",
    "# requires kaggle account\n",
    "zip_fips_crosswalk = pd.read_csv('zipcodes-county-fips-crosswalk.zip', dtype={'ZIP': str, 'STCOUNTYFP': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_fips_crosswalk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_fips_dict = dict()\n",
    "for i, row in zip_fips_crosswalk.iterrows():\n",
    "    zip_fips_dict[row['ZIP']] = row['STCOUNTYFP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bad(?) zips\n",
    "for zip_code in list(zip_coords_dict.keys()):\n",
    "    if zip_code not in zip_fips_dict:\n",
    "        zip_coords_dict.pop(zip_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_coords_dict = dict()\n",
    "for zip_code, coords in zip_coords_dict.items():\n",
    "    fips = zip_fips_dict[zip_code]\n",
    "    try:\n",
    "        fips_coords_dict[fips].extend(coords)\n",
    "    except KeyError:\n",
    "        fips_coords_dict[fips] = coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIPS Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = pd.read_fwf('https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt',\n",
    "   colspecs=[(0,11), (12, 20), (21, 30), (31, 37), (38, 40), (41, 71), (72, 75), (76, 79), (80, 85)],\n",
    "   header=None,\n",
    "   names=['ID', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'STATE', 'NAME', 'GSN FLAG', 'HCN/CRN FLAG', 'WMO ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only includes HCN stations in the continental United States\n",
    "stations_df = stations_df[stations_df['ID'].str.startswith('US') & (stations_df['HCN/CRN FLAG'] == 'HCN')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/stations_df.pkl', 'wb') as file:\n",
    "    pickle.dump(stations_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stations_to_fips(fips):\n",
    "    coords = fips_coords_dict[fips]\n",
    "    fips_stations = list()\n",
    "    time.sleep(0)\n",
    "    for i, station in stations_df.iterrows():\n",
    "        station_coord = (station['LATITUDE'], station['LONGITUDE'])\n",
    "        time.sleep(0)\n",
    "        for coord in coords:\n",
    "            dist = distance(station_coord, coord).miles\n",
    "            if dist <= 50:\n",
    "                fips_stations.append(station['ID'])\n",
    "                break\n",
    "    return (fips, fips_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fips_codes = list(fips_coords_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = ipyparallel.Client()\n",
    "view = rc.load_balanced_view()\n",
    "\n",
    "with rc[:].sync_imports():\n",
    "    import time\n",
    "\n",
    "rc[:].push(dict(\n",
    "    distance=distance.distance,\n",
    "    fips_coords_dict=fips_coords_dict,\n",
    "    stations_df=stations_df\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = view.map_async(add_stations_to_fips, fips_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fips_codes = len(fips_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctrl + enter on this cell to see progress without blocking kernel\n",
    "print(f'Time Elapsed: {datetime.timedelta(seconds=math.ceil(ar.elapsed))}')\n",
    "print(f'FIPS Codes Completed: {ar.progress}/{num_fips_codes}')\n",
    "print(f'Completed {round(ar.serial_time/ar.wall_time, 2)}x faster than serial computation' if ar.ready() else 'Still Running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will block the kernel; interrupt the kernel to unblock\n",
    "ar.wait_interactive()\n",
    "print(f'Completed {round(ar.serial_time/ar.wall_time, 2)}x faster than serial computation' if ar.ready() else 'Still Running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what to do with counties with no stations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict from coordinate to county in US, None if not in US, or an error object\n",
    "fips_stations_dict = dict()\n",
    "for fips, stations in ar:\n",
    "    fips_stations_dict[fips] = stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/fips_stations_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(fips_stations_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to ID for fast lookup\n",
    "stations_df.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict from fips to dict from station to inverse average distance\n",
    "fips_stations_inverse_distances_dict = dict()\n",
    "num_fips_codes = len(fips_stations_dict)\n",
    "for i, (fips, stations) in enumerate(fips_stations_dict.items()):\n",
    "    num_stations = len(stations)\n",
    "    for j, station in enumerate(stations):\n",
    "        print(f'fips: {i+1}/{num_fips_codes}\\nstations: {j+1}/{num_stations}')\n",
    "        clear_output(wait=True)\n",
    "        distances = list()\n",
    "        station_coord = (stations_df.loc[station]['LATITUDE'], stations_df.loc[station]['LONGITUDE'])\n",
    "        for coord in fips_coords_dict[fips]:\n",
    "            distances.append(distance.distance(station_coord, coord).miles)\n",
    "        distances = np.array(distances, dtype=float)\n",
    "        inverse_distances = np.reciprocal(distances)\n",
    "        try:\n",
    "            fips_stations_inverse_distances_dict[fips][station] = np.mean(inverse_distances)\n",
    "        except KeyError:\n",
    "            fips_stations_inverse_distances_dict[fips] = {station: np.mean(inverse_distances)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/fips_stations_inverse_distances_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(fips_stations_inverse_distances_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
